{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCSoRRgS59YS"
      },
      "source": [
        "# Importing Json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP_rXJLUv4hF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFlNPJyiQCQ2"
      },
      "outputs": [],
      "source": [
        "annotations_file = 'path/Train_dataset.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuelvFx7QCQ2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "data = json.load(open(annotations_file))\n",
        "\n",
        "# Revisar las clases unicas del Json\n",
        "unique_classes = set()\n",
        "for image in data['images']:\n",
        "    for obj in image['objects']:\n",
        "        unique_classes.add(obj['class'])\n",
        "print(\"Unique Classes:\", unique_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypQ3WGsEE7QJ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Contar las instancias de los objetos en el json\n",
        "class_counts = Counter()\n",
        "for image in data[\"images\"]:\n",
        "    for obj in image[\"objects\"]:\n",
        "        class_counts[obj[\"class\"]] += 1\n",
        "\n",
        "class_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A_qDqrnyszn"
      },
      "source": [
        "# Pocosi_24042024_100epocs\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ncqqOLEGOZ4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "annotations_file = 'path/Train_dataset.json'\n",
        "\n",
        "def class_text_to_int(class_text):\n",
        "    class_mapping = { \"dump_truck\": 1,\"person\": 2,\"excavator\": 3,\"loader\": 4,\"mixer_truck\": 5,\"steamroller\": 6}\n",
        "    return class_mapping.get(class_text, None)\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None):\n",
        "        with open(annotations_file) as f:\n",
        "            self.img_labels = json.load(f)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform if transform is not None else transforms.Compose([\n",
        "            transforms.ToTensor(),  # Converts PIL images to PyTorch tensors\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels['images'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels['images'][idx]['image_path'])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        objects = self.img_labels['images'][idx]['objects']\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for obj in objects:\n",
        "            xmin, ymin, xmax, ymax = obj['bbox'].values()\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            labels.append(class_text_to_int(obj['class']))\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def get_model(num_classes):\n",
        "    model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, accumulation_steps=4):\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "    optimizer.zero_grad()\n",
        "    for i, (images, targets) in enumerate(data_loader):\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        with autocast():\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values()) / accumulation_steps\n",
        "        scaler.scale(losses).backward()\n",
        "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "    print(f\"Epoch #{epoch} perdida: {losses.item()}\")\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    num_classes = 7\n",
        "    dataset = CustomDataset(annotations_file, 'path/Images_etiquetadas', transform=None)\n",
        "\n",
        "    data_loader = DataLoader(dataset, batch_size=24, shuffle=True, collate_fn=collate_fn, num_workers=20, pin_memory=True)\n",
        "\n",
        "    model = get_model(num_classes).to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
        "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    num_epochs = 100\n",
        "    for epoch in range(num_epochs):\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch)\n",
        "        torch.save(model.state_dict(), f'path/model_epoch_{epoch}.pth',)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Zc7WqwO18k"
      },
      "source": [
        "# Validation Section"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluaci칩n de modelos del conjunto de datos de prueba\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def class_text_to_int(class_text):\n",
        "    class_mapping = {\n",
        "        \"dump_truck\": 1,\n",
        "        \"person\": 2,\n",
        "        \"excavator\": 3,\n",
        "        \"loader\": 4,\n",
        "        \"mixer_truck\": 5,\n",
        "        \"steamroller\": 6\n",
        "    }\n",
        "    return class_mapping.get(class_text, None)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None):\n",
        "        with open(annotations_file) as f:\n",
        "            self.img_labels = json.load(f)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform if transform is not None else transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels['images'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_info = self.img_labels['images'][idx]\n",
        "        img_path = os.path.join(self.img_dir, img_info['image_path'])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for obj in img_info['objects']:\n",
        "            bbox = obj['bbox']\n",
        "            boxes.append([bbox['x_min'], bbox['y_min'], bbox['x_max'], bbox['y_max']])\n",
        "            labels.append(class_text_to_int(obj['class']))\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def bbox_iou(box1, box2):\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "    inter_area = max(x2 - x1, 0) * max(y2 - y1, 0)\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    iou = inter_area / float(box1_area + box2_area - inter_area)\n",
        "    return iou\n",
        "\n",
        "def get_model(num_classes):\n",
        "    model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "Pocosi_24042024_30epocs = 'path/model.pth'\n",
        "model = get_model(num_classes=7)\n",
        "model.load_state_dict(torch.load(Pocosi_24042024_30epocs, map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define your test dataset here\n",
        "test_annotations_file = 'path/test_dataset.json'\n",
        "images_path = 'path/Images_etiquetadas'\n",
        "test_dataset = CustomDataset(test_annotations_file, images_path)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "ious = []\n",
        "true_positives, false_positives, false_negatives = 0, 0, 0\n",
        "iou_threshold = 0.7\n",
        "\n",
        "for images, targets in test_loader:\n",
        "    images = [image.to(device) for image in images]\n",
        "    outputs = model(images)\n",
        "    for i, output in enumerate(outputs):\n",
        "        predicted_boxes = output['boxes'].data.cpu().numpy()\n",
        "        predicted_labels = output['labels'].data.cpu().numpy()\n",
        "        target_boxes = targets[i]['boxes'].data.cpu().numpy()\n",
        "        target_labels = targets[i]['labels'].data.cpu().numpy()\n",
        "\n",
        "        matched_gt = set()\n",
        "        for pb_idx, predicted_box in enumerate(predicted_boxes):\n",
        "            for tb_idx, target_box in enumerate(target_boxes):\n",
        "                iou = bbox_iou(predicted_box, target_box)\n",
        "                if iou > iou_threshold and tb_idx not in matched_gt:\n",
        "                    matched_gt.add(tb_idx)\n",
        "                    ious.append(iou)\n",
        "                    if predicted_labels[pb_idx] == target_labels[tb_idx]:\n",
        "                        true_positives += 1\n",
        "                    else:\n",
        "                        false_positives += 1\n",
        "                    break\n",
        "            else:\n",
        "                false_positives += 1\n",
        "        false_negatives += len(target_boxes) - len(matched_gt)\n",
        "\n",
        "average_iou = sum(ious) / len(ious) if ious else 0\n",
        "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
        "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
        "\n",
        "print(f\"Average IoU: {average_iou:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n"
      ],
      "metadata": {
        "id": "k8b_WVFW4OUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFHy0GTlW_pn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Funci칩n para graficar las im치genes\n",
        "def draw_boxes(ax, boxes, labels, color):\n",
        "    for box, label in zip(boxes, labels):\n",
        "        x1, y1, x2, y2 = box\n",
        "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, y1, label, verticalalignment='bottom', horizontalalignment='left', color=color, fontsize=12)\n",
        "\n",
        "# Figura de visualizaci칩n\n",
        "fig, axes = plt.subplots(20, 1, figsize=(30, 50))  # 1 row, 5 columns\n",
        "image_counter = 0\n",
        "\n",
        "test_annotations_file = 'path/test_dataset.json'\n",
        "\n",
        "\n",
        "# Carga datos\n",
        "test_dataset = CustomDataset(test_annotations_file, images_path, transform=None)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "for images, targets in test_loader:\n",
        "    images = [image.to(device) for image in images]\n",
        "    outputs = model(images)\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        if image_counter >= 20:\n",
        "            break\n",
        "\n",
        "        ax = axes[image_counter]\n",
        "\n",
        "\n",
        "        img = F.to_pil_image(image.cpu())\n",
        "        ax.imshow(img)\n",
        "\n",
        "        # Cajas reales\n",
        "        target_boxes = targets[i]['boxes'].cpu().numpy()\n",
        "        target_labels = [str(label) for label in targets[i]['labels'].cpu().numpy()]\n",
        "        draw_boxes(ax, target_boxes, target_labels, 'blue')\n",
        "\n",
        "        # Cajas predichas\n",
        "        predicted_boxes = outputs[i]['boxes'].detach().cpu().numpy()\n",
        "        predicted_scores = outputs[i]['scores'].detach().cpu().numpy()\n",
        "        predicted_labels = [str(label) for label in outputs[i]['labels'].detach().cpu().numpy()]\n",
        "\n",
        "\n",
        "        high_score_idxs = [idx for idx, score in enumerate(predicted_scores) if score > 0.7]\n",
        "        predicted_boxes = predicted_boxes[high_score_idxs]\n",
        "        predicted_labels = [predicted_labels[idx] for idx in high_score_idxs]\n",
        "\n",
        "        draw_boxes(ax, predicted_boxes, predicted_labels, 'red')\n",
        "\n",
        "        ax.axis('off')\n",
        "        image_counter += 1\n",
        "\n",
        "    if image_counter >= 20:\n",
        "        break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        " \"dump_truck\": 1,\n",
        "        \"person\": 2,\n",
        "        \"excavator\": 3,\n",
        "        \"loader\": 4,\n",
        "        \"mixer_truck\": 5,\n",
        "        \"streamroller : 6'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}